\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{abbrvnat}
\citation{kozbelt2010theories}
\citation{runco1990theories}
\citation{guilford1978alternate}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Alternate uses test}{1}{section.2}\protected@file@percent }
\newlabel{sec:alternate_uses}{{2}{1}{Alternate uses test}{section.2}{}}
\citation{plucker2010assessment}
\citation{runco2012standard}
\citation{weisberg2016creativity}
\citation{simonton2018defining}
\citation{stevenson2022puttinggpt3screativityalternative}
\citation{Haase_2023}
\citation{organisciak2023beyond}
\citation{goes2023pushing}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related work}{2}{section.3}\protected@file@percent }
\newlabel{sec:related_work}{{3}{2}{Related work}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{2}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{2}{Method}{section.4}{}}
\citation{zhao2024assessing}
\citation{creativehuddle}
\citation{zhao2024assessingunderstandingcreativitylarge}
\citation{zhao2024assessing}
\citation{zhao2024assessingunderstandingcreativitylarge}
\citation{lin-2004-rouge}
\citation{papineni-etal-2002-bleu}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The evaluator prompt: \texttt  {instruction}, \texttt  {reference\_answer}, and \texttt  {rubric} are parameters to the prompt\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{sec3:eval_AUT_prompt}{{1}{3}{The evaluator prompt: \texttt {instruction}, \texttt {reference\_answer}, and \texttt {rubric} are parameters to the prompt\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The scoring rubric used by the evaluator LLM\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{sec3:scoring_rubric}{{2}{3}{The scoring rubric used by the evaluator LLM\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Designing the evaluation prompt}{3}{subsection.4.1}\protected@file@percent }
\newlabel{sec:design_eval_prompt}{{4.1}{3}{Designing the evaluation prompt}{subsection.4.1}{}}
\citation{li2024leveraginglargelanguagemodels}
\citation{gao2024llmbasednlgevaluationcurrent}
\citation{chiang-lee-2023-large}
\citation{zheng2023judgingllmasajudgemtbenchchatbot}
\citation{li2024leveraginglargelanguagemodels}
\citation{wang2024pandalmautomaticevaluationbenchmark}
\citation{dubois2023alpacafarm}
\citation{kim2024prometheusinducingfinegrainedevaluation}
\citation{kim2024prometheusinducingfinegrainedevaluation}
\citation{yang2024largemultimodalmodelsuncover}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}LLMs as machine evaluators}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:llms_as_eval}{{4.2}{4}{LLMs as machine evaluators}{subsection.4.2}{}}
\citation{guilford1978alternate}
\citation{creativehuddle}
\citation{davebirss}
\citation{brown2020languagemodelsfewshotlearners}
\citation{ravent√≥s2023pretrainingtaskdiversityemergence}
\citation{dong2024surveyincontextlearning}
\citation{agarwal2024manyshotincontextlearning}
\citation{lee2024promptinglargelanguagemodels}
\citation{xiao2024humanaicollaborativeessayscoring}
\citation{zheng2023judgingllmasajudgemtbenchchatbot}
\citation{Sun_2024}
\citation{AUTdataset}
\citation{Sun_2024}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Huggingface pre-trained LLMs, and the URL where they can be found, used in AUT experiment.\relax }}{5}{table.caption.3}\protected@file@percent }
\newlabel{sec2:modelstable}{{1}{5}{Huggingface pre-trained LLMs, and the URL where they can be found, used in AUT experiment.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Designing the responses prompt}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:designing_prompt}{{4.3}{5}{Designing the responses prompt}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Bowl and paperclip dataset}{5}{section.5}\protected@file@percent }
\newlabel{sec:bowl_and_paperclip}{{5}{5}{Bowl and paperclip dataset}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Sample sizes, quality and scores}{5}{subsection.5.1}\protected@file@percent }
\citation{Sun_2024}
\citation{Sun_2024}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The responses prompt: \texttt  {object} is a parameter to the prompt \relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{bowl_and_paperclip:fig1}{{3}{6}{The responses prompt: \texttt {object} is a parameter to the prompt \relax }{figure.caption.4}{}}
\newlabel{LLM_AUT_prompt}{{3}{6}{The responses prompt: \texttt {object} is a parameter to the prompt \relax }{figure.caption.4}{}}
\citation{AUTdataset}
\citation{prometheus_huggingface}
\citation{kim2024prometheus2opensource}
\citation{Sullivan_2013}
\citation{norman2010likert}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Experimental protocol}{7}{subsection.5.2}\protected@file@percent }
\newlabel{sec:protocol}{{5.2}{7}{Experimental protocol}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Results}{7}{subsection.5.3}\protected@file@percent }
\newlabel{sec:results}{{5.3}{7}{Results}{subsection.5.3}{}}
\citation{stevenson2022puttinggpt3screativityalternative}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Human AUT evaluation on the bowl and paperclip objects. The responses were randomly sampled into groups of $40$, with the remainder forming a smaller sample. They were presented to the machine evaluator; these are the outputs. Responses which scored less than $3$ were filtered out, so we have different numbers of samples for the different objects.\relax }}{8}{table.caption.5}\protected@file@percent }
\newlabel{sec_results:human_results}{{2}{8}{Human AUT evaluation on the bowl and paperclip objects. The responses were randomly sampled into groups of $40$, with the remainder forming a smaller sample. They were presented to the machine evaluator; these are the outputs. Responses which scored less than $3$ were filtered out, so we have different numbers of samples for the different objects.\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Large language model evaluations on the bowl and paperclip objects.\relax }}{8}{table.caption.6}\protected@file@percent }
\newlabel{sec_results:model_results}{{3}{8}{Large language model evaluations on the bowl and paperclip objects.\relax }{table.caption.6}{}}
\citation{stevenson2022dataset}
\citation{stevenson2022dataset}
\citation{stevenson2022puttinggpt3screativityalternative}
\citation{stevenson2022puttinggpt3screativityalternative}
\@writefile{toc}{\contentsline {section}{\numberline {6}Fork, book, and tin can dataset}{9}{section.6}\protected@file@percent }
\newlabel{sec:fork_book_tincan}{{6}{9}{Fork, book, and tin can dataset}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sample sizes, quality and scores}{9}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Experimental protocol}{9}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Results}{9}{subsection.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Large language model evaluations on the book, fork and tin can objects. Mean and standard deviation of the AUT scores are given for each model on each object, then the overall mean and standard deviation for all LLMs on all objects.\relax }}{10}{table.caption.7}\protected@file@percent }
\newlabel{book_fork_tincan:model_results}{{4}{10}{Large language model evaluations on the book, fork and tin can objects. Mean and standard deviation of the AUT scores are given for each model on each object, then the overall mean and standard deviation for all LLMs on all objects.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Human evaluations on the book, fork and tin can objects. Mean and standard deviation of the AUT scores are given for each object, then the overall mean and standard deviation on all objects.\relax }}{10}{table.caption.8}\protected@file@percent }
\newlabel{book_fork_tincan:human_results}{{5}{10}{Human evaluations on the book, fork and tin can objects. Mean and standard deviation of the AUT scores are given for each object, then the overall mean and standard deviation on all objects.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{10}{section.7}\protected@file@percent }
\bibdata{LLM_vs_human_AUT_comparison}
\bibcite{agarwal2024manyshotincontextlearning}{{1}{2024}{{Agarwal et~al.}}{{Agarwal, Singh, Zhang, Bohnet, Rosias, Chan, Zhang, Anand, Abbas, Nova, Co-Reyes, Chu, Behbahani, Faust, and Larochelle}}}
\bibcite{davebirss}{{2}{2024}{{Birss}}{{}}}
\bibcite{brown2020languagemodelsfewshotlearners}{{3}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}}}
\bibcite{chiang-lee-2023-large}{{4}{2023}{{Chiang and Lee}}{{}}}
\bibcite{creativehuddle}{{5}{2024}{{CreativeHuddle}}{{}}}
\bibcite{dong2024surveyincontextlearning}{{6}{2024}{{Dong et~al.}}{{Dong, Li, Dai, Zheng, Ma, Li, Xia, Xu, Wu, Chang, Sun, Li, and Sui}}}
\bibcite{dubois2023alpacafarm}{{7}{2023}{{Dubois et~al.}}{{Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto}}}
\bibcite{gao2024llmbasednlgevaluationcurrent}{{8}{2024}{{Gao et~al.}}{{Gao, Hu, Ruan, Pu, and Wan}}}
\bibcite{goes2023pushing}{{9}{2023}{{G{\'o}es et~al.}}{{G{\'o}es, Volpe, Sawicki, Grses, and Watson}}}
\bibcite{guilford1978alternate}{{10}{1978}{{Guilford et~al.}}{{Guilford, Christensen, Merrifield, and Wilson}}}
\bibcite{Haase_2023}{{11}{2023}{{Haase and Hanel}}{{}}}
\bibcite{kim2024prometheusinducingfinegrainedevaluation}{{12}{2024{a}}{{Kim et~al.}}{{Kim, Shin, Cho, Jang, Longpre, Lee, Yun, Shin, Kim, Thorne, and Seo}}}
\bibcite{kim2024prometheus2opensource}{{13}{2024{b}}{{Kim et~al.}}{{Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo}}}
\bibcite{kozbelt2010theories}{{14}{2010}{{Kozbelt et~al.}}{{Kozbelt, Beghetto, and Runco}}}
\bibcite{lee2024promptinglargelanguagemodels}{{15}{2024}{{Lee et~al.}}{{Lee, Cai, Meng, Wang, and Wu}}}
\bibcite{prometheus_huggingface}{{16}{2024}{{Leonov}}{{}}}
\bibcite{li2024leveraginglargelanguagemodels}{{17}{2024}{{Li et~al.}}{{Li, Xu, Shen, Xu, Gu, Lai, Tao, and Ma}}}
\bibcite{lin-2004-rouge}{{18}{2004}{{Lin}}{{}}}
\bibcite{norman2010likert}{{19}{2010}{{Norman}}{{}}}
\bibcite{organisciak2023beyond}{{20}{2023}{{Organisciak et~al.}}{{Organisciak, Acar, Dumas, and Berthiaume}}}
\bibcite{papineni-etal-2002-bleu}{{21}{2002}{{Papineni et~al.}}{{Papineni, Roukos, Ward, and Zhu}}}
\bibcite{plucker2010assessment}{{22}{2010}{{Plucker}}{{}}}
\bibcite{ravent√≥s2023pretrainingtaskdiversityemergence}{{23}{2023}{{Ravent√≥s et~al.}}{{Ravent√≥s, Paul, Chen, and Ganguli}}}
\bibcite{runco1990theories}{{24}{1990}{{Runco and Albert}}{{}}}
\bibcite{runco2012standard}{{25}{2012}{{Runco and Jaeger}}{{}}}
\bibcite{simonton2018defining}{{26}{2018}{{Simonton}}{{}}}
\bibcite{stevenson2022dataset}{{27}{2022{a}}{{Stevenson et~al.}}{{Stevenson, Smal, Baas, Grasman, and van~der Maas}}}
\bibcite{stevenson2022puttinggpt3screativityalternative}{{28}{2022{b}}{{Stevenson et~al.}}{{Stevenson, Smal, Baas, Grasman, and van~der Maas}}}
\bibcite{Sullivan_2013}{{29}{2013}{{Sullivan and Artino}}{{}}}
\bibcite{AUTdataset}{{30}{2023}{{Sun et~al.}}{{Sun, Gu, Myers, and Yuan}}}
\bibcite{Sun_2024}{{31}{2024}{{Sun et~al.}}{{Sun, Gu, Myers, and Yuan}}}
\bibcite{wang2024pandalmautomaticevaluationbenchmark}{{32}{2024}{{Wang et~al.}}{{Wang, Yu, Zeng, Yang, Wang, Chen, Jiang, Xie, Wang, Xie, Ye, Zhang, and Zhang}}}
\bibcite{weisberg2016creativity}{{33}{2016}{{Weisberg}}{{}}}
\bibcite{xiao2024humanaicollaborativeessayscoring}{{34}{2024}{{Xiao et~al.}}{{Xiao, Ma, Song, Xu, Zhang, Wang, and Fu}}}
\bibcite{yang2024largemultimodalmodelsuncover}{{35}{2024}{{Yang et~al.}}{{Yang, Li, Dong, Xia, and Sui}}}
\bibcite{zhao2024assessingunderstandingcreativitylarge}{{36}{2024{a}}{{Zhao et~al.}}{{Zhao, Zhang, Li, Huang, Guo, Peng, Hao, Wen, Hu, Du, Guo, Li, and Chen}}}
\bibcite{zhao2024assessing}{{37}{2024{b}}{{Zhao et~al.}}{{Zhao, Zhang, Li, Huang, Guo, Peng, Hao, Wen, Hu, Du, et~al.}}}
\bibcite{zheng2023judgingllmasajudgemtbenchchatbot}{{38}{2023}{{Zheng et~al.}}{{Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica}}}
\gdef \@abspage@last{12}
