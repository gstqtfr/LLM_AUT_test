@book{runco1990theories,
  title={{Theories of creativity}},
  author={Runco, Mark A and Albert, Robert S},
  volume={990},
  year={1990},
  publisher={Sage Publications London}
}

@article{guilford1978alternate,
  title={{Alternate Uses: Manual of instructions and interpretations. Palo Alto}},
  author={Guilford, JP and Christensen, PR and Merrifield, PR and Wilson, RC},
  journal={CA: Mind Garden},
  year={1978}
}

@article{beatyetal2020,
    author = {Beaty, Roger and Johnson, Dan},
    year = {2020},
    month = {02},
    pages = {},
    journal = {nwvps},
    title = {{Automating Creativity Assessment with SemDis: An Open Platform for Computing Semantic Distance}},
    doi = {10.31234/osf.io/nwvps}
}

@article{runco2012standard,
  title={{The standard definition of creativity}},
  author={Runco, Mark A and Jaeger, Garrett J},
  journal={Creativity research journal},
  volume={24},
  number={1},
  pages={92--96},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{gilhooly2023ai,
  title={{AI vs humans in the AUT: simulations to LLMs.}},
  author={Gilhooly, Ken},
  journal={Journal of Creativity},
  pages={100071},
  year={2023},
  publisher={Elsevier}
}

 @article{Kim_2006, title={{Can We Trust Creativity Tests? A Review of the Torrance Tests of Creative Thinking (TTCT)}}, volume={18}, ISSN={1532-6934}, url={http://dx.doi.org/10.1207/s15326934crj1801_2}, DOI={10.1207/s15326934crj1801_2}, number={1}, journal={Creativity Research Journal}, publisher={Informa UK Limited}, author={Kim, Kyung Hee}, year={2006}, month=jan, pages={3–14} }

@article{guilford1967nature,
  title={{The nature of human intelligence}},
  author={Guilford, Joy Paul},
  journal={New York: Macgraw Hill},
  year={1967}
}

@misc{AUTdataset,
    author = {Sun, Luning and Gu, Hongyi and Myers, Rebecca and Yuan, Zheng},
    title = {Cambridge-AUT-dataset},
    year = 2023,
    url ={https://github.com/ghydsgaaa/Cambridge-AUT-dataset},
    urldate = {2023-12}
}

@article{Sun_2024, 
    title={{A New Dataset and Method for Creativity Assessment Using the Alternate Uses Task}}, 
    ISBN={9789819700653}, 
    ISSN={1865-0937}, 
    url={http://dx.doi.org/10.1007/978-981-97-0065-3_9}, 
    DOI={10.1007/978-981-97-0065-3_9}, 
    journal={Intelligent Computers, Algorithms, and Applications}, 
    publisher={Springer Nature Singapore}, 
    author={Sun, Luning and Gu, Hongyi and Myers, Rebecca and Yuan, Zheng}, 
    year={2024}, 
    pages={125–138} 
}

@article{Sullivan_2013, 
    title={Analyzing and Interpreting Data From Likert-Type Scales}, 
    volume={5}, 
    ISSN={1949-8349}, 
    url={http://dx.doi.org/10.4300/JGME-5-4-18}, 
    DOI={10.4300/jgme-5-4-18}, 
    number={4}, 
    journal={Journal of Graduate Medical Education}, 
    publisher={Journal of Graduate Medical Education}, 
    author={Sullivan, Gail M. and Artino, Anthony R.}, 
    year={2013}, month=dec, 
    pages={541–542} 
}

@misc{stevenson2022puttinggpt3screativityalternative,
      title={Putting GPT-3's Creativity to the (Alternative Uses) Test}, 
      author={Claire Stevenson and Iris Smal and Matthijs Baas and Raoul Grasman and Han van der Maas},
      year={2022},
      eprint={2206.08932},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2206.08932}, 
}

@article{norman2010likert,
  title={Likert scales, levels of measurement and the “laws” of statistics},
  author={Norman, Geoff},
  journal={Advances in health sciences education},
  volume={15},
  pages={625--632},
  year={2010},
  publisher={Springer}
}

@inproceedings{chiang-lee-2023-large,
    title = "Can Large Language Models Be an Alternative to Human Evaluations?",
    author = "Chiang, Cheng-Han  and
      Lee, Hung-yi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.870",
    doi = "10.18653/v1/2023.acl-long.870",
    pages = "15607--15631",
    abstract = "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
}

@article{zhao2024assessing,
  title={{Assessing and understanding creativity in large language models}},
  author={Zhao, Yunpu and Zhang, Rui and Li, Wenyi and Huang, Di and Guo, Jiaming and Peng, Shaohui and Hao, Yifan and Wen, Yuanbo and Hu, Xing and Du, Zidong and others},
  journal={arXiv preprint arXiv:2401.12491},
  year={2024}
}

@misc{zhao2024assessingunderstandingcreativitylarge,
      title={{Assessing and Understanding Creativity in Large Language Models}}, 
      author={Yunpu Zhao and Rui Zhang and Wenyi Li and Di Huang and Jiaming Guo and Shaohui Peng and Yifan Hao and Yuanbo Wen and Xing Hu and Zidong Du and Qi Guo and Ling Li and Yunji Chen},
      year={2024},
      eprint={2401.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.12491}, 
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{liu-etal-2023-g,
    title = "{G}-Eval: {NLG} Evaluation using Gpt-4 with Better Human Alignment",
    author = "Liu, Yang  and
      Iter, Dan  and
      Xu, Yichong  and
      Wang, Shuohang  and
      Xu, Ruochen  and
      Zhu, Chenguang",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.153",
    doi = "10.18653/v1/2023.emnlp-main.153",
    pages = "2511--2522",
    abstract = "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@article{kozbelt2010theories,
  title={{Theories of creativity}},
  author={Kozbelt, Aaron and Beghetto, Ronald A and Runco, Mark A},
  journal={The Cambridge handbook of creativity},
  volume={2},
  pages={20--47},
  year={2010}
}



@misc{kim2024prometheusinducingfinegrainedevaluation,
      title={{Prometheus: Inducing Fine-grained Evaluation Capability in Language Models}}, 
      author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
      year={2024},
      eprint={2310.08491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.08491}, 
}

@misc{kim2024prometheus2opensource,
      title={{Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models}}, 
      author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2405.01535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01535}, 
}

@misc{prometheus_huggingface,
    title={{Prometheus-7b-v2.0}},
    author={Seva Leonov},
    year={2024},
    url={https://huggingface.co/vsevolodl/prometheus-7b-v2.0-GGUF}
}

@misc{dubois2023alpacafarm,
  title={{AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}}, 
  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year={2023},
  eprint={2305.14387},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{simonton2018defining,
  title={{Defining creativity: Don't we also need to define what is not creative?}},
  author={Simonton, Dean Keith},
  journal={The Journal of Creative Behavior},
  volume={52},
  number={1},
  pages={80--90},
  year={2018},
  publisher={Wiley Online Library}
}

@misc{stevenson2022dataset,
    title={{Dataset for: Putting GPT-3's Creativity to the (Alternative Uses) Test}}, 
    author={Claire Stevenson and Iris Smal and Matthijs Baas and Raoul Grasman and Han van der Maas},
    year={2022},
    url={https://osf.io/vmk3c/},
}

@article{Haase_2023,
   title={{Artificial muses: Generative artificial intelligence chatbots have risen to human-level creativity}},
   volume={33},
   ISSN={2713-3745},
   url={http://dx.doi.org/10.1016/j.yjoc.2023.100066},
   DOI={10.1016/j.yjoc.2023.100066},
   number={3},
   journal={Journal of Creativity},
   publisher={Elsevier BV},
   author={Haase, Jennifer and Hanel, Paul H.P.},
   year={2023},
   month=dec, pages={100066} 
}

@article{organisciak2023beyond,
  title={{Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models}},
  author={Organisciak, Peter and Acar, Selcuk and Dumas, Denis and Berthiaume, Kelly},
  journal={Thinking Skills and Creativity},
  volume={49},
  pages={101356},
  year={2023},
  publisher={Elsevier}
}

@article{goes2023pushing,
  title={{Pushing GPT’s creativity to its limits: Alternative uses and Torrance tests}},
  author={G{\'o}es, Luis Fabricio and Volpe, Marco and Sawicki, Piotr and Grses, Marek and Watson, Jacob},
  year={2023},
  publisher={University of Leicester}
}

@book{weisberg2016creativity,
  title={{Creativity: Understanding innovation in problem solving, science, invention, and the arts}},
  author={Weisberg, Robert W},
  year={2016},
  publisher={John Wiley \& Sons}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {{AlpacaEval: An Automatic Evaluator of Instruction-following Models}},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={{BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi}},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@misc{wang2024pandalmautomaticevaluationbenchmark,
      title={{PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization}}, 
      author={Yidong Wang and Zhuohao Yu and Zhengran Zeng and Linyi Yang and Cunxiang Wang and Hao Chen and Chaoya Jiang and Rui Xie and Jindong Wang and Xing Xie and Wei Ye and Shikun Zhang and Yue Zhang},
      year={2024},
      eprint={2306.05087},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05087}, 
}

@misc{dubois2024alpacafarmsimulationframeworkmethods,
      title={{AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}}, 
      author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2305.14387},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14387}, 
}

@inproceedings{kour2014real,
  title={{Real-time segmentation of on-line handwritten arabic script}},
  author={Kour, George and Saabne, Raid},
  booktitle={{Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on}},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={{Fast classification of handwritten on-line Arabic characters}},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={{Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications}},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@misc{ye2024flaskfinegrainedlanguagemodel,
      title={{FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets}}, 
      author={Seonghyeon Ye and Doyoung Kim and Sungdong Kim and Hyeonbin Hwang and Seungone Kim and Yongrae Jo and James Thorne and Juho Kim and Minjoon Seo},
      year={2024},
      eprint={2307.10928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.10928}, 
}

@misc{li2024leveraginglargelanguagemodels,
      title={{Leveraging Large Language Models for NLG Evaluation: Advances and Challenges}}, 
      author={Zhen Li and Xiaohan Xu and Tao Shen and Can Xu and Jia-Chen Gu and Yuxuan Lai and Chongyang Tao and Shuai Ma},
      year={2024},
      eprint={2401.07103},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.07103}, 
}

@misc{gao2024llmbasednlgevaluationcurrent,
      title={{LLM-based NLG Evaluation: Current Status and Challenges}}, 
      author={Mingqi Gao and Xinyu Hu and Jie Ruan and Xiao Pu and Xiaojun Wan},
      year={2024},
      eprint={2402.01383},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01383}, 
}

@misc{yang2024largemultimodalmodelsuncover,
      title={{Can Large Multimodal Models Uncover Deep Semantics Behind Images?}}, 
      author={Yixin Yang and Zheng Li and Qingxiu Dong and Heming Xia and Zhifang Sui},
      year={2024},
      eprint={2402.11281},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11281}, 
}

@book{plucker2010assessment,
  title={{Assessment of creativity}},
  author={Plucker, JA},
  year={2010},
  publisher={The Cambridge Handbook of Creativity/Cambridge University Press}
}

@misc{creativehuddle,
    year = {2024},
    author = {CreativeHuddle},
    url = {https://www.creativehuddle.co.uk/post/the-alternative-uses-test}
}

@misc{davebirss,
    year = {2024},
    author = {Dave Birss},
    url = {https://davebirss.com/altuses/}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={{Language Models are Few-Shot Learners}}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{raventós2023pretrainingtaskdiversityemergence,
      title={{Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression}}, 
      author={Allan Raventós and Mansheej Paul and Feng Chen and Surya Ganguli},
      year={2023},
      eprint={2306.15063},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.15063}, 
}

@misc{dong2024surveyincontextlearning,
      title={{A Survey on In-context Learning}}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@misc{lee2024promptinglargelanguagemodels,
      title={{Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization}}, 
      author={Sanwoo Lee and Yida Cai and Desong Meng and Ziyang Wang and Yunfang Wu},
      year={2024},
      eprint={2404.04941},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.04941}, 
}

@misc{xiao2024humanaicollaborativeessayscoring,
      title={{Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs}}, 
      author={Changrong Xiao and Wenxing Ma and Qingping Song and Sean Xin Xu and Kunpeng Zhang and Yufang Wang and Qi Fu},
      year={2024},
      eprint={2401.06431},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06431}, 
}

@misc{agarwal2024manyshotincontextlearning,
      title={{Many-Shot In-Context Learning}}, 
      author={Rishabh Agarwal and Avi Singh and Lei M. Zhang and Bernd Bohnet and Luis Rosias and Stephanie Chan and Biao Zhang and Ankesh Anand and Zaheer Abbas and Azade Nova and John D. Co-Reyes and Eric Chu and Feryal Behbahani and Aleksandra Faust and Hugo Larochelle},
      year={2024},
      eprint={2404.11018},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.11018}, 
}